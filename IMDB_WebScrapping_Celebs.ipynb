{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMDB_WebScrapping_Celebs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/invinciblesohail/Webscrapping-IMDB/blob/main/IMDB_WebScrapping_Celebs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4R4RZ6cbKLe"
      },
      "source": [
        "#@ author = Sohail Ahmed\n",
        "\n",
        "# Final Project\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKuVJ2JzcZp8"
      },
      "source": [
        "# url source\n",
        "\n",
        "url = \"https://www.imdb.com/title/tt6452574/?ref_=fn_al_tt_1\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ0D1g6Jc5wr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ddd8f361-8f38-4ea9-e354-52fb7aaf2461"
      },
      "source": [
        "import requests\n",
        "import lxml.html as lh\n",
        "import pandas as pd\n",
        "page = requests.get(url)\n",
        "\n",
        "type(page)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "requests.models.Response"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0i8opH78dMdi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "4360b33d-b5c2-4d19-fbf8-b5f834d0d755"
      },
      "source": [
        "\n",
        "html = page.text\n",
        "#print(html)\n",
        "doc = lh.fromstring(page.content)\n",
        "print(doc)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Element html at 0x7fd2604d9228>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_vrfEvDdVRm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "6372cb38-16b4-4794-e52c-f70630ba14be"
      },
      "source": [
        "#Import BeautifulSoup from bs4\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "# Create a BeautifulSoup object from the HTML\n",
        "\n",
        "soup = BeautifulSoup(html, \"html.parser\")\n",
        "type(soup)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "bs4.BeautifulSoup"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3smq3YEP1-a"
      },
      "source": [
        "#print(soup.prettify())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Okk78mN7hmgf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "aca713fb-3d4f-4641-ef9e-65b8ff9b8757"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVcetLgmQz88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a04c72e2-cf2c-48cd-be2b-ce0ef9f3de29"
      },
      "source": [
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "import re\n",
        "\n",
        "\n",
        "movie_rating = []\n",
        "movie_poster = []\n",
        "directors = []\n",
        "writers = []\n",
        "stars = []\n",
        "language =[]\n",
        "country=[]\n",
        "releasedate = []\n",
        "taglines = []\n",
        "genres = []\n",
        "certificate = []\n",
        "cast =[]\n",
        "\n",
        "rating = soup.find('span',itemprop = \"ratingValue\").string\n",
        "movie_rating.append(rating)\n",
        "\n",
        "for container in soup.find_all('div', class_ = \"poster\"):\n",
        "    movie_poster.append(container.img.get('title'))\n",
        "\n",
        "for container in soup.find_all('div', class_ = \"credit_summary_item\"):\n",
        "\n",
        "    if (container.find('h4',class_ = \"inline\").string == \"Director:\" ):\n",
        "        directors.append(container.a.string)\n",
        "    if (container.find('h4',class_ = \"inline\").string == \"Writers:\"):\n",
        "        for writer in container.find_all('a'):\n",
        "            writers.append(writer.string)\n",
        "    if (container.find('h4',class_ = \"inline\").string == \"Star:\"):\n",
        "        stars.append(container.a.string)\n",
        "    elif (container.find('h4',class_ = \"inline\").string == \"Stars:\"):\n",
        "        for star in container.find_all('a'):\n",
        "            stars.append(star.string)\n",
        "\n",
        "\n",
        "\n",
        "details_container = soup.find_all('div', class_=\"txt-block\")\n",
        "for container in details_container:\n",
        "  if(container.find('h4', class_ =\"inline\") is not None):\n",
        "    if (container.h4.string == \"Country:\"):\n",
        "      country.append(container.a.string)\n",
        "    if (container.h4.string == \"Language:\"):\n",
        "      language.append(container.a.string)\n",
        "    if (container.h4.string == \"Release Date:\"):\n",
        "      text1 = container.text.strip(\"\\n, Release Date:\")\n",
        "      releasedate.append(text1[:12])\n",
        "    if (container.h4.string == \"Taglines:\"):\n",
        "      text1 = word_tokenize(container.text)\n",
        "      text2 = TreebankWordDetokenizer().detokenize(text1[2:])\n",
        "      taglines.append(text2)\n",
        "      #print(container.text[10:20])\n",
        "    if(container.h4.string == \"Certificate:\"):\n",
        "      certificate.append(container.span.text)\n",
        "      \n",
        "      \n",
        "article_container = soup.find_all('div', class_ = \"see-more inline canwrap\")\n",
        "for container in article_container:\n",
        "  if(container.h4.text == \"Genres:\"):\n",
        "    text1 = word_tokenize(container.text)\n",
        "    text2 = TreebankWordDetokenizer().detokenize(text1[2:])\n",
        "    cleanString = re.sub('\\W+',', ', text2 )\n",
        "    genres.append(cleanString)\n",
        "\n",
        "tr_elements = doc.xpath('//tr')\n",
        "\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Movie title : \", soup.title.string)\n",
        "print(\"Rating :\", movie_rating)\n",
        "print(\"Movie Poster name : \",movie_poster)\n",
        "print(\"Directors :\", directors)\n",
        "print(\"Writers : \", writers)\n",
        "print(\"Start cast : \", stars)\n",
        "print(\"Country : \", country)\n",
        "print(\"Language : \",language)\n",
        "print(\"Release Date : \",releasedate)\n",
        "print(\"Tag Lines :\",taglines)\n",
        "print(\"Genres : \",genres)\n",
        "print(\"Certificate : \", certificate)\n",
        "print(cast)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Movie title :  Sanju (2018) - IMDb\n",
            "Rating : ['7.7']\n",
            "Movie Poster name :  ['Sanju Poster']\n",
            "Directors : ['Rajkumar Hirani']\n",
            "Writers :  ['Abhiruchi Chand', 'Rajkumar Hirani', '1 more credit']\n",
            "Start cast :  ['Ranbir Kapoor', 'Paresh Rawal', 'Manisha Koirala', 'See full cast & crew']\n",
            "Country :  ['India']\n",
            "Language :  ['Hindi']\n",
            "Release Date :  ['29 June 2018']\n",
            "Tag Lines : ['One Man . . . Many Lives']\n",
            "Genres :  ['Biography, Comedy, Drama']\n",
            "Certificate :  ['TV-MA']\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}